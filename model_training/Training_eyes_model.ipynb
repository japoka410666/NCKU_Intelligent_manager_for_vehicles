{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Conv2D,Dropout,MaxPooling2D,Flatten\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "import pathlib\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2423 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "#img_gen = ImageDataGenerator(validation_split=0.3,rescale=1/255,dtype='float32')\n",
    "\n",
    "#load images and labels\n",
    "img_gen = ImageDataGenerator(dtype='float32')\n",
    "train_image_gen = img_gen.flow_from_directory(\"eyes\",target_size=(60,60),color_mode='grayscale',batch_size=1)\n",
    "\n",
    "images_array = train_image_gen[0][0]\n",
    "labels_array = train_image_gen[0][1]\n",
    "for i in range(1,2423):\n",
    "    images_array = np.append(images_array,train_image_gen[i][0])\n",
    "    labels_array = np.append(labels_array,train_image_gen[i][1])\n",
    "\n",
    "#print(labels_array)\n",
    "#print(images_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#images preprocessing\n",
    "images_array = images_array.reshape(2423,60,60,1)\n",
    "\n",
    "images_array = images_array.astype('float32')\n",
    "\n",
    "images_array = (images_array - 128.0) / 128 #range of input date become [-1:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " ...\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]]\n",
      "(2423, 2)\n"
     ]
    }
   ],
   "source": [
    "#label preprocessing\n",
    "labels_done = []\n",
    "labels_array = labels_array.reshape(2423,2)\n",
    "print(labels_array)\n",
    "\n",
    "for i in range(len(labels_array)):\n",
    "    for j in range(0,2):\n",
    "        if(labels_array[i][j]):\n",
    "            labels_done = np.append(labels_done,j)\n",
    "\n",
    "test_labels = labels_done\n",
    "\n",
    "labels_done = to_categorical(labels_done, 2, dtype = 'float32')\n",
    "print(labels_done.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2423, 2)\n",
      "(2423, 60, 60, 1)\n"
     ]
    }
   ],
   "source": [
    "print(labels_done.shape)\n",
    "print(images_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 100, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = cv2.imread(\"eyes/ClosedFace/closed_eye_0001.jpg_face_1.jpg\")\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape=(60,60,1)\n",
    "\n",
    "#model = Sequential()\n",
    "#model.add(Conv2D(128,(3,3),input_shape=input_shape,activation=\"relu\"))\n",
    "#model.add(MaxPooling2D((2,2)))\n",
    "#model.add(Conv2D(64,(3,3),activation=\"relu\"))\n",
    "#model.add(MaxPooling2D((2,2)))\n",
    "#model.add(Dropout(.4))\n",
    "#model.add(Conv2D(32,(3,3),activation=\"relu\"))\n",
    "#model.add(MaxPooling2D((2,2)))\n",
    "#model.add(Flatten())\n",
    "#model.add(Dense(1000,activation=\"relu\"))\n",
    "#model.add(Dropout(.4))\n",
    "#model.add(Dense(128,activation=\"relu\"))\n",
    "#model.add(Dropout(.4))\n",
    "#model.add(Dense(32,activation=\"relu\"))\n",
    "#model.add(Dropout(.4))\n",
    "#model.add(Dense(2,activation=\"softmax\"))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#model = Sequential([\n",
    "#    Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape),\n",
    "#    Conv2D(32, kernel_size=(3, 3), activation='relu'),\n",
    "#    MaxPooling2D(pool_size=(2, 2)),\n",
    "#    Dropout(0.25),\n",
    "    \n",
    "#    Conv2D(64, kernel_size=(3, 3), padding='same', activation='relu'),\n",
    "#    Conv2D(64, kernel_size=(3, 3),activation='relu'),\n",
    "#    MaxPooling2D(pool_size=(2, 2)),\n",
    "#    Dropout(0.25),\n",
    "    \n",
    "#    Flatten(),\n",
    "#    Dense(128, activation='relu'),\n",
    "#    Dropout(0.5),\n",
    "#    Dense(2, activation='softmax')\n",
    "#    \n",
    "#])\n",
    "\n",
    "model = Sequential([\n",
    "    Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape),   # 1st layer\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Conv2D(32, (3, 3), activation='relu'),                                        # 2nd layer\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),                                        # 3rd layer\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "#    Dropout(0.25),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "#    Dropout(0.5),\n",
    "    Dense(2, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 58, 58, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 29, 29, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 27, 27, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               204928    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 233,250\n",
      "Trainable params: 233,250\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\",loss=\"categorical_crossentropy\",metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ClosedFace': 0, 'OpenFace': 1}\n",
      "2423\n"
     ]
    }
   ],
   "source": [
    "print(train_image_gen.class_indices)\n",
    "num_steps_per_epoch = len(train_image_gen) ##batch_size\n",
    "print(num_steps_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "2423/2423 [==============================] - 12s 5ms/step - loss: 0.4805 - accuracy: 0.7582\n",
      "Epoch 2/15\n",
      "2423/2423 [==============================] - 11s 5ms/step - loss: 0.2024 - accuracy: 0.9236\n",
      "Epoch 3/15\n",
      "2423/2423 [==============================] - 11s 5ms/step - loss: 0.1408 - accuracy: 0.9484\n",
      "Epoch 4/15\n",
      "2423/2423 [==============================] - 12s 5ms/step - loss: 0.1167 - accuracy: 0.9558\n",
      "Epoch 5/15\n",
      "2423/2423 [==============================] - 11s 5ms/step - loss: 0.0888 - accuracy: 0.9657\n",
      "Epoch 6/15\n",
      "2423/2423 [==============================] - 11s 5ms/step - loss: 0.0721 - accuracy: 0.9748\n",
      "Epoch 7/15\n",
      "2423/2423 [==============================] - 10s 4ms/step - loss: 0.0610 - accuracy: 0.9802\n",
      "Epoch 8/15\n",
      "2423/2423 [==============================] - 11s 4ms/step - loss: 0.0642 - accuracy: 0.9798\n",
      "Epoch 9/15\n",
      "2423/2423 [==============================] - 10s 4ms/step - loss: 0.0528 - accuracy: 0.9798\n",
      "Epoch 10/15\n",
      "2423/2423 [==============================] - 10s 4ms/step - loss: 0.0377 - accuracy: 0.9868\n",
      "Epoch 11/15\n",
      "2423/2423 [==============================] - 10s 4ms/step - loss: 0.0528 - accuracy: 0.9802\n",
      "Epoch 12/15\n",
      "2423/2423 [==============================] - 10s 4ms/step - loss: 0.0402 - accuracy: 0.9864\n",
      "Epoch 13/15\n",
      "2423/2423 [==============================] - 11s 5ms/step - loss: 0.0394 - accuracy: 0.9868\n",
      "Epoch 14/15\n",
      "2423/2423 [==============================] - 11s 4ms/step - loss: 0.0361 - accuracy: 0.9897\n",
      "Epoch 15/15\n",
      "2423/2423 [==============================] - 11s 5ms/step - loss: 0.0427 - accuracy: 0.9839\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1d49d681df0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(images_array,labels_done,epochs = 15,steps_per_epoch=num_steps_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save weights of this model\n",
    "model.save_weights('my_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#without optimized\n",
    "#converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "#converted_model = converter.convert()\n",
    "#import pathlib\n",
    "# Save the model.\n",
    "#generated_dir = pathlib.Path(\"generated/\")\n",
    "#generated_dir.mkdir(exist_ok=True, parents=True)\n",
    "#converted_model_file = \"eyes_model.tflite\"\n",
    "#converted_model_file.write_bytes(converted_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load weights to this TensorFlow model\n",
    "model.load_weights('my_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Adaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From C:\\Adaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\Japoka\\AppData\\Local\\Temp\\tmpbtzyfhb_\\assets\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#convert model into TFLM format\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type = tf.int8\n",
    "converter.inference_output_type = tf.int8\n",
    "\n",
    "#preprocess representative images\n",
    "preprocessed_test_images = images_array\n",
    "preprocessed_test_images = tf.cast(preprocessed_test_images, tf.float32)\n",
    "tflite_ds = tf.data.Dataset.from_tensor_slices((preprocessed_test_images)).batch(1) #construct a dataset \n",
    "\n",
    "def representative_data_gen():\n",
    "    for input_value in tflite_ds.take(100):\n",
    "        yield [input_value]\n",
    "    \n",
    "converter.representative_dataset = representative_data_gen\n",
    "\n",
    "#convert model\n",
    "converted_model = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "243072"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pathlib\n",
    "generated_dir = pathlib.Path(\"generated/\")\n",
    "generated_dir.mkdir(exist_ok=True, parents=True)\n",
    "converted_model_file = generated_dir/\"eyes_model_int8.tflite\"\n",
    "converted_model_file.write_bytes(converted_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter = tf.lite.Interpreter(model_path=str(converted_model_file))\n",
    "interpreter.allocate_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00390625\n",
      "-128\n"
     ]
    }
   ],
   "source": [
    "max_samples = 100\n",
    "scale, zero_point = interpreter.get_output_details()[0]['quantization']\n",
    "print(scale)\n",
    "print(zero_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper function to evaluate the TF Lite model using \"test\" dataset.\n",
    "def evaluate_model(interpreter):\n",
    "    input_index = interpreter.get_input_details()[0][\"index\"]\n",
    "    output_index = interpreter.get_output_details()[0][\"index\"]\n",
    "    scale, zero_point = interpreter.get_output_details()[0]['quantization']\n",
    "\n",
    "    prediction_values = []\n",
    "    output_buffer = []\n",
    "    #only consider the result having strong confidence(?%) to reduce lossing accuracy\n",
    "    higher_threshold = 0\n",
    "    lowwer_threshold = 0\n",
    "    \n",
    "    for test_image in preprocessed_test_images[:max_samples]:\n",
    "        # Pre-processing: add batch dimension, quantize and convert inputs to int8 to match with\n",
    "        # the model's input data format.\n",
    "        test_image = np.expand_dims(test_image, axis=0) #.astype(np.float32)\n",
    "        test_image = np.int8(test_image / scale + zero_point)\n",
    "        interpreter.set_tensor(input_index, test_image)\n",
    "\n",
    "        interpreter.invoke()\n",
    "\n",
    "        # Find the answer with highest probability\n",
    "        output = interpreter.tensor(output_index)\n",
    "        output_buffer = np.append(output_buffer,output()[0][1])\n",
    "        result = np.argmax(output()[0])\n",
    "        prediction_values.append(result)\n",
    "        \n",
    "    accurate_count_k = 0\n",
    "    num_data_used = 0\n",
    "    \n",
    "    for index in range(len(prediction_values)):\n",
    "        if(output_buffer[index]>higher_threshold or output_buffer[index]<lowwer_threshold):\n",
    "            num_data_used +=1\n",
    "            if prediction_values[index] == test_labels[index]:\n",
    "                accurate_count_k +=1\n",
    "                \n",
    "    #print(num_data_used)    \n",
    "\n",
    "    accuracy = accurate_count_k * 1.0 / num_data_used\n",
    "    \n",
    "    return accuracy * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51.0%\n"
     ]
    }
   ],
   "source": [
    "print(str(evaluate_model(interpreter)) + \"%\")\n",
    "#evaluate_model(interpreter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
